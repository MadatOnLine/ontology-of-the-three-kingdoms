{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP PROJECT - ONTOLOGY OF THE THREE KINGDOMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for NLP techniques\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import collections\n",
    "\n",
    "#Utils\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Easy-to-use character's name lists generator from a specific noisy file.\n",
    "#BUG - Some names may have \" \" at the end, must be removed or load them will fail!\n",
    "def generateNamesList():\n",
    "    file = open(\"Lists/Main Characters.txt\", \"r\");\n",
    "    charText = file.readlines();\n",
    "    file.close();\n",
    "    allList = [];\n",
    "    ladyList = [];\n",
    "    menList = [];\n",
    "    aliasList = [];\n",
    "    aliasDict = {};\n",
    "    \n",
    "    print(\"Generating lists...\");\n",
    "    \n",
    "    for line in charText[1:]:\n",
    "        words = line.split(\",\");\n",
    "        if \" \" in words[len(words)-1]:\n",
    "                    words.pop(len(words)-1);\n",
    "\n",
    "        if words[0] != \"\\n\":\n",
    "            if \"lady\" in words[0] or \"empress\" in words[0] or \"diaochan\" in words[0]:\n",
    "                ladyList.append(words[0]);\n",
    "                allList.append(words[0]);\n",
    "            else:\n",
    "                if \"(\" in words[0]:\n",
    "                    names = words[0].split(\"(\");\n",
    "                    courtesy = names[1].split(\")\")[0];\n",
    "                    aliasDict[names[0]] = courtesy;\n",
    "                    aliasList.append(names[0]);\n",
    "                    aliasList.append(courtesy);\n",
    "                    allList.append(courtesy);\n",
    "                    menList.append(names[0]);\n",
    "                    allList.append(names[0]);\n",
    "                else:\n",
    "                    menList.append(words[0]);\n",
    "                    allList.append(words[0]);\n",
    "                    \n",
    "#Removing final blank line\n",
    "    allList.pop(len(allList)-1);\n",
    "    menList.pop(len(menList)-1);\n",
    "    ladyList.pop(len(ladyList)-1);\n",
    "    aliasList.pop(len(aliasList)-1);\n",
    "    \n",
    "    np.savetxt(\"Lists/allNames.txt\", allList, fmt=\"%s\");\n",
    "    print(\"allNames.txt created!\");\n",
    "    \n",
    "    np.savetxt(\"Lists/menList.txt\", menList, fmt=\"%s\");\n",
    "    print(\"menList.txt created!\");\n",
    "    \n",
    "    np.savetxt(\"Lists/ladyList.txt\", ladyList, fmt=\"%s\");\n",
    "    print(\"ladyList.txt created!\");\n",
    "    \n",
    "    np.savetxt(\"Lists/aliasesList.txt\", aliasList, fmt=\"%s\");\n",
    "    print(\"aliasesList.txt created!\");\n",
    "    \n",
    "    return allList, menList, ladyList, aliasList, aliasDict;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list from files, you will not have aliasDict, but you probably don't need it.\n",
    "def loadLists():\n",
    "    file = open(\"Lists/allNames.txt\", \"r\");\n",
    "    allList = file.read().split(\"\\n\");\n",
    "    file = open(\"Lists/menList.txt\", \"r\");\n",
    "    menList = file.read().split(\"\\n\");\n",
    "    file = open(\"Lists/ladyList.txt\", \"r\");\n",
    "    ladyList = file.read().split(\"\\n\");\n",
    "    file = open(\"Lists/aliasesList.txt\", \"r\");\n",
    "    aliasList = file.read().split(\"\\n\");\n",
    "    file.close();\n",
    "    return allList, menList, ladyList, aliasList;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (should) Retrieve all phrases related to the character:\n",
    "# the actual sentece, the one before and the one after\n",
    "# his/her name appears get merge together.\n",
    "def infoXchar(charSurn, charName, sourceText):\n",
    "    rightSurn = False;\n",
    "    bookmark = 0;\n",
    "    match = 0;\n",
    "    end = len(sourceText);\n",
    "    text = sourceText.copy();\n",
    "    wantedInfo = [];\n",
    "    \n",
    "    for line in text:\n",
    "        words = line.split();\n",
    "        for word in words:\n",
    "            if word == charName and rightSurn:\n",
    "                rightSurn = False;\n",
    "                match += 1;\n",
    "                if bookmark == 0:\n",
    "                    wantedInfo.append(text[0] + text[1] + text[2]);\n",
    "                elif bookmark == end-1:\n",
    "                    wantedInfo.append(text[bookmark-2] + text[bookmark-1] + text[bookmark])\n",
    "                else:\n",
    "                    wantedInfo.append(text[bookmark-1] + text[bookmark] + text[bookmark+1])\n",
    "            elif word == charSurn:\n",
    "                    rightSurn = True;\n",
    "            else:\n",
    "                rightSurn = False;\n",
    "        bookmark += 1;\n",
    "    print(\"Found \" + str(match) + \" phrases.\");\n",
    "    return wantedInfo;\n",
    "#Version 2 will also check for aliases, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (should) Retrieve all phrases where a certain\n",
    "# item appears. The output is an array of string\n",
    "# where each cell is formed by the actual sentece\n",
    "# which you find the item on, the one before and\n",
    "# the one after.\n",
    "def infoXitem(item, text):\n",
    "    bookmark = 0;\n",
    "    match = 0;\n",
    "    end = len(sourceText);\n",
    "    text = sourceText.copy();\n",
    "    wantedInfo = [];\n",
    "    \n",
    "    for line in text:\n",
    "        words = line.split();\n",
    "        for word in words:\n",
    "            if word == item:\n",
    "                match += 1;\n",
    "                if bookmark == 0:\n",
    "                    wantedInfo.append(text[0] + text[1] + text[2]);\n",
    "                elif bookmark == end-1:\n",
    "                    wantedInfo.append(text[bookmark-2] + text[bookmark-1] + text[bookmark])\n",
    "                else:\n",
    "                    wantedInfo.append(text[bookmark-1] + text[bookmark] + text[bookmark+1])\n",
    "        bookmark += 1;\n",
    "        \n",
    "    print(\"Found \" + str(match) + \" phrases.\");\n",
    "    return wantedInfo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a dictionary (#appearance, word)\n",
    "def frequentWord(sourceText, uniqueWords = set(), wordCount = {}):\n",
    "# sourceText should be an array of string\n",
    "    infoText = sourceText.copy();\n",
    "    previousToken = \"x\";\n",
    "    for block in infoText:\n",
    "        tokens = word_tokenize(block);\n",
    "        for word in tokens:\n",
    "            if word not in stop_words:\n",
    "                if previousToken[0].isupper() and word[0].isupper():\n",
    "                    wordCount[previousToken + \" \" + word] = wordCount.get(previousToken + \" \" + word, 0)+1;\n",
    "                    uniqueWords.add(previousToken + \" \" + word)\n",
    "                else:\n",
    "                    wordCount[word] = wordCount.get(word, 0)+1;\n",
    "                    uniqueWords.add(word);\n",
    "            previousToken = word;\n",
    "\n",
    "    wordFreq = []\n",
    "    for key, value in wordCount.items():\n",
    "        wordFreq.append((value, key))\n",
    "    wordFreq.sort(reverse=True)\n",
    "\n",
    "    return wordFreq, uniqueWords, wordCount;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check semantic field from common words to understand\n",
    "# what kind of role our character played.\n",
    "def computeRole(wordFreq, wantData = False):\n",
    "    warriorness = 0;\n",
    "    politicness = 0;\n",
    "    base = len(wordFreq);\n",
    "\n",
    "    for term in wordFreq:\n",
    "        if \" \" in term[1]: #Removing (most) entities\n",
    "            base -= 1;\n",
    "        if term[1].lower() in warWords:\n",
    "            warriorness += term[0];\n",
    "        if term[1].lower() in poliWords:\n",
    "            politicness += term[0];\n",
    "\n",
    "    warriorness = warriorness/(base+1);\n",
    "    politicness = politicness/(base+1);\n",
    "    \n",
    "    if wantData:\n",
    "        return warriorness, politicness;\n",
    "    else:\n",
    "        print(\"Warriorness: \" + str(warriorness) + \" | Politicness: \" + str(politicness));\n",
    "        return 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Helping Lists & Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allocating list\n",
    "allList = [];\n",
    "menList = [];\n",
    "ladyList = [];\n",
    "aliasList = [];\n",
    "\n",
    "if Path('Lists/allNames.txt').is_file():\n",
    "    print(\"Character's name list already computed.\");\n",
    "    allList, menList, ladyList, aliasList = loadLists();\n",
    "else:\n",
    "    allList, menList, ladyList, aliasList, aliasDict = generateNamesList();\n",
    "\n",
    "#if Path('Lists/eventNames.txt').is_file():\n",
    "#    print(\"Event's name list ready.\")\n",
    "#else:\n",
    "    #generateEventsList();\n",
    "    #file = open(\"Lists/Main Events.txt\", \"r\");\n",
    "    #eventText = file.readlines();\n",
    "    \n",
    "file = open(\"Lists/warWords.txt\", \"r\");\n",
    "warWords = file.read().split(\"\\n\");\n",
    "file = open(\"Lists/poliWords.txt\", \"r\");\n",
    "poliWords = file.read().split(\"\\n\");\n",
    "file.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting Stop Words\n",
    "lib_stopWords = set(stopwords.words('english'));\n",
    "stop_words = lib_stopWords.copy();\n",
    "stop_words.add(\"but\");\n",
    "\n",
    "for word in lib_stopWords:\n",
    "    upWord = word[0].upper() + word[1:];\n",
    "    stop_words.add(upWord);\n",
    "\n",
    "for sign in punctuation:\n",
    "    stop_words.add(sign);\n",
    "    \n",
    "stop_words.add(\"''\");\n",
    "stop_words.add(\"``\");\n",
    "stop_words.remove(\".\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Chapters you Want to Explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading files\n",
    "file = open(\"Dataset/chap001-004.txt\", \"r\");\n",
    "#file = open(\"Dataset/chap005-012.txt\", \"r\");\n",
    "chaps = file.read();\n",
    "file.close();\n",
    "text = chaps.split(\".\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Searching!\n",
    "Let's analyze the novel starting from a character or entity you want.<br>\n",
    "Since you may not know which characters are in the chosen chapters, let's compute a list.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering & computing values for each main character\n",
    "warDict = {};\n",
    "poliDict = {};\n",
    "warRes = [];\n",
    "poliRes = [];\n",
    "\n",
    "for char in allList:\n",
    "    pers = char.split(\" \");\n",
    "    cSur = (pers[0][0].upper() + pers[0][1:]);\n",
    "    if pers[len(pers)-1] == \" \":\n",
    "        pers[len(pers)-1] = \"\";\n",
    "    if len(pers) == 1:\n",
    "        cNam = \"\";\n",
    "    else:\n",
    "        cNam = (pers[1][0].upper() + pers[1][1:]);\n",
    "    print(cSur + \" \" + cNam);\n",
    "    charInfo = infoXchar(cSur, cNam, text);\n",
    "    wordFreq, uniqueWords, wordCount = frequentWord(charInfo, uniqueWords = set(), wordCount = {});\n",
    "    \n",
    "#Estimate 'warriorness' and 'politicness' of the character\n",
    "    warriorness = 0;\n",
    "    politicness = 0;\n",
    "    base = len(wordFreq);\n",
    "\n",
    "    for term in wordFreq:\n",
    "        if \" \" in term[1]: #Removing (most) entities\n",
    "            base -= 1;\n",
    "        if term[1].lower() in warWords:\n",
    "            warriorness += term[0];\n",
    "        if term[1].lower() in poliWords:\n",
    "            politicness += term[0];\n",
    "\n",
    "    warriorness = warriorness/(base+1);\n",
    "    politicness = politicness/(base+1);\n",
    "    if warriorness != 0 and politicness != 0:\n",
    "        data = cSur + \" \" + cNam + \", \" + str(warriorness) + \", \" + str(politicness);\n",
    "\n",
    "    warDict[cSur + \" \" + cNam]= warriorness;\n",
    "    poliDict[cSur + \" \" + cNam]= politicness;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list will tell you who is present in the chapter and how likely he/she could be a warrior or a politician."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting results\n",
    "for key, value in warDict.items():\n",
    "    warRes.append((value, key));\n",
    "    \n",
    "for key, value in poliDict.items():\n",
    "    poliRes.append((value, key));\n",
    "    \n",
    "warRes.sort(reverse=True);\n",
    "poliRes.sort(reverse=True);\n",
    "\n",
    "print(\"WARRIOR\")\n",
    "for val, char in warRes:\n",
    "    if val != 0:\n",
    "        print(char + \": \" + str(val));\n",
    "\n",
    "print(\"\\nPOLITICIAN\")\n",
    "for val, char in poliRes:\n",
    "    if val != 0:\n",
    "        print(char + \": \" + str(val));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Character or Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = infoXchar(\"Sun\", \"Jian\", text); #Example Sun Jian\n",
    "#info = infoXitem(\"ITEM\", text);\n",
    "wordFreq, uniqueWords, wordCount = frequentWord(info, uniqueWords = set(), wordCount = {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have info about who we want, let's answer to the following questions in order to enrich the ontology, keeping in mind the fact that useful informations may show up while looking at something else.\n",
    "<ul>\n",
    "    <li>What has he done? -> Which Events has he been involved in?</li>\n",
    "    <li>What kind of relationships did he develop? -> With whom?</li>\n",
    "    <li>What did he achieve? -> Which Title has he manage to get?</li>\n",
    "</ul>\n",
    "We could just read all retrieved blocks, but they could be many so let's proceed with order...\n",
    "\n",
    "## WHEN and WHERE we are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in wordFreq[:20]:\n",
    "    print(entry);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the year and the event our character is acting in by looking at:\n",
    "<ul>\n",
    "    <li>Dates, if we are like enought to find one;</li>\n",
    "    <li>Characters, by looking when they lived;</li>\n",
    "    <li>Factions, if he/she is cited with something we know we can use info from that other entity to better understand our character.</li>\n",
    "</ul>\n",
    "Were you able to answer the question above? Probably not, but don't be afraid: at least we should have got some names, right?\n",
    "\n",
    "## Entity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in wordFreq[:100]:\n",
    "    if \" \" in entry[1] or entry[1].isupper():\n",
    "        print(entry);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we definetly have something we can work on, maybe we can even answer to the question above!<br>\n",
    "We know who our character interact with, let's pick one person or item from the list above and try to answer to the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"ENTITY\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW and WHY did he/she interacts with that particular character?\n",
    "or\n",
    "## HOW that Item is related to our character?\n",
    "\n",
    "While you reading those blocks, you should keep an eye on:\n",
    "<ul>\n",
    "    <li>Cities, because for us European spotting Chinese-location just by looking at names can be hard;</li>\n",
    "    <li>Dialogues, to understand the relationships between characters;</li>\n",
    "    <li>Titles, but you probably have to guess it by context;</li>\n",
    "    <li>Attitude towards Faction, to understand the course of the event.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0;\n",
    "for block in info:\n",
    "    if entity in block:\n",
    "        i += 1;\n",
    "        print(\"CITATION \" + str(i));\n",
    "        print(block);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were you able to understand something more about the character? I sincerely hope so!<br>\n",
    "If not keep doing this with other meaningful entities or words that could be related to a job and in the end you'll learn a lot about your character, other entities, the events their acting in etc...All this little block hides what you need to enrich the ontology!<br>\n",
    "If you really can't understand anything, you can always read all blocks related to your character. Doing so would be faster than reading the whole chapters, but still it could takes some times because you may have tons of blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0;\n",
    "for block in info:\n",
    "    i += 1;\n",
    "    print(\"Citation \" + str(i));\n",
    "    print(block);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
